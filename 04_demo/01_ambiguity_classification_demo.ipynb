{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP9K2lu9WdJmLX4X0awkgeF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jihyeyu33/LLM-Interactive-Clarification/blob/main/04_demo/01_ambiguity_classification_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "845skeoiiNM1"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "!pip install -q transformers accelerate bitsandbytes gradio peft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9uUUuy4dP1av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(f\"GPU 사용 가능: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU 이름: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")"
      ],
      "metadata": {
        "id": "aQw25BtYiX03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 설정\n",
        "CLASSIFY_BASE_MODEL = \"microsoft/Phi-4-mini-reasoning\"\n",
        "CLASSIFY_LORA_ADAPTER = \"jyering/classify-ambig\"\n",
        "\n",
        "QUESTION_GEN_BASE_MODEL = \"microsoft/Phi-4-mini-reasoning\"\n",
        "QUESTION_GEN_LORA_ADAPTER = \"Dayeoni/question-generator-dpo\"\n",
        "\n",
        "ANSWER_GEN_MODEL = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# 분류 모델 시스템 프롬프트\n",
        "CLASSIFY_SYSTEM_PROMPT = \"\"\"You are an AI system that determines if the question requires clarification and classifies the ambiguity.\n",
        "\n",
        "Task:\n",
        "1. Determine if the question requires clarification: clear(no clarification needed) or ambiguous(clarification needed)\n",
        "2. Classify the ambiguity:\n",
        " - If question is clear, set category=NONE and subclass=NONE\n",
        " - If question is ambiguous, classify category and subclass\n",
        "\n",
        "Output format: category|subclass\n",
        "\n",
        "Categories:\n",
        "- EM (Epistemic Misalignment): Questions with unfamiliar entities or self-contradictions\n",
        "- LA (Linguistic Ambiguity): Questions with lexical or semantic ambiguity\n",
        "- AO (Aleatoric Output): Questions with missing contextual information causing confusion\n",
        "- NONE: Clear questions that don't require clarification\n",
        "\n",
        "Subclasses:\n",
        "For EM:\n",
        "- UNF (UNFAMILIAR): Query contains unfamiliar entities or facts\n",
        "- CONT (CONTRADICTION): Query contains self-contradictions\n",
        "\n",
        "For LA:\n",
        "- LEX (LEXICAL): Query contains terms with multiple meanings\n",
        "- SEM (SEMANTIC): Query lacks context leading to multiple interpretations\n",
        "\n",
        "For AO:\n",
        "- WHOM: Query output contains confusion due to missing personal elements\n",
        "- WHEN: Query output contains confusion due to missing temporal elements\n",
        "- WHERE: Query output contains confusion due to missing spatial elements\n",
        "- WHAT: Query output contains confusion due to missing task-specific elements\n",
        "\"\"\"\n",
        "\n",
        "# 질문 생성 모델 시스템 프롬프트\n",
        "QUESTION_GEN_SYSTEM_PROMPT = \"\"\"You are an AI that generates a single, concise clarifying question when a user's query is ambiguous.\n",
        "\n",
        "Task:\n",
        "Generate exactly one clarifying question based on the ambiguity type.\n",
        "\n",
        "Output format: One clarifying question\n",
        "\n",
        "Categories:\n",
        "- EM (Epistemic Misalignment): Questions with unfamiliar entities or self-contradictions\n",
        "- LA (Linguistic Ambiguity): Questions with lexical or semantic ambiguity\n",
        "- AO (Aleatoric Output): Questions with missing contextual information causing confusion\n",
        "\n",
        "Subclasses:\n",
        "For EM:\n",
        "- UNF (UNFAMILIAR): Query contains unfamiliar entities or facts\n",
        "- CONT (CONTRADICTION): Query contains self-contradictions\n",
        "\n",
        "For LA:\n",
        "- LEX (LEXICAL): Query contains terms with multiple meanings\n",
        "- SEM (SEMANTIC): Query lacks context leading to multiple interpretations\n",
        "\n",
        "For AO:\n",
        "- WHOM: Query output contains confusion due to missing personal elements\n",
        "- WHEN: Query output contains confusion due to missing temporal elements\n",
        "- WHERE: Query output contains confusion due to missing spatial elements\n",
        "- WHAT: Query output contains confusion due to missing task-specific elements\"\"\"\n",
        "\n",
        "print(\"설정 완료\")"
      ],
      "metadata": {
        "id": "vbpRb55Vigh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "\n",
        "# 분류 모델\n",
        "print(\"분류 모델 로딩 중...\")\n",
        "classify_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    CLASSIFY_BASE_MODEL,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "classify_base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    CLASSIFY_BASE_MODEL,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "classify_model = PeftModel.from_pretrained(\n",
        "    classify_base_model,\n",
        "    CLASSIFY_LORA_ADAPTER\n",
        ")\n",
        "classify_model = classify_model.merge_and_unload()\n",
        "print(\"분류 모델 로드 완료\\n\")\n",
        "\n",
        "# 질문 생성 모델\n",
        "print(\"질문 생성 모델 로딩 중...\")\n",
        "question_gen_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    QUESTION_GEN_BASE_MODEL,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "question_gen_base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    QUESTION_GEN_BASE_MODEL,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "question_gen_model = PeftModel.from_pretrained(\n",
        "    question_gen_base_model,\n",
        "    QUESTION_GEN_LORA_ADAPTER\n",
        ")\n",
        "question_gen_model = question_gen_model.merge_and_unload()\n",
        "print(\"질문 생성 모델 로드 완료\\n\")\n",
        "\n",
        "# 답변 생성 모델\n",
        "print(\"답변 생성 모델 로딩 중...\")\n",
        "answer_gen_tokenizer = AutoTokenizer.from_pretrained(ANSWER_GEN_MODEL)\n",
        "answer_gen_model = AutoModelForCausalLM.from_pretrained(\n",
        "    ANSWER_GEN_MODEL,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "print(\"답변 생성 모델 로드 완료\\n\")\n",
        "\n",
        "print(f\"모든 모델 로드 완료 (GPU 메모리: {torch.cuda.memory_allocated() / 1024**3:.2f} GB)\")"
      ],
      "metadata": {
        "id": "k9BqhTCFiiRG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_ambiguity(query):\n",
        "    \"\"\"분류 함수\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": CLASSIFY_SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "\n",
        "    input_ids = classify_tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(classify_model.device)\n",
        "\n",
        "    outputs = classify_model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        do_sample=False,\n",
        "        pad_token_id=classify_tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    response = classify_tokenizer.decode(\n",
        "        outputs[0][input_ids.shape[1]:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    return response.strip()\n",
        "\n",
        "\n",
        "def generate_clarifying_question(classification, query):\n",
        "    \"\"\"명확화 질문 생성\"\"\"\n",
        "    user_input = f\"[{classification}] {query}\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": QUESTION_GEN_SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": user_input}\n",
        "    ]\n",
        "\n",
        "    input_ids = question_gen_tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(question_gen_model.device)\n",
        "\n",
        "    outputs = question_gen_model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=100,\n",
        "        do_sample=False,\n",
        "        pad_token_id=question_gen_tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    response = question_gen_tokenizer.decode(\n",
        "        outputs[0][input_ids.shape[1]:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    return response.strip()\n",
        "\n",
        "\n",
        "def generate_answer(query):\n",
        "    \"\"\"답변 생성\"\"\"\n",
        "    messages = [{\"role\": \"user\", \"content\": query}]\n",
        "\n",
        "    input_ids = answer_gen_tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(answer_gen_model.device)\n",
        "\n",
        "    outputs = answer_gen_model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=200,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=answer_gen_tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    response = answer_gen_tokenizer.decode(\n",
        "        outputs[0][input_ids.shape[1]:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    return response.strip()\n",
        "\n",
        "\n",
        "def is_ambiguous(classification):\n",
        "    \"\"\"모호성 판단\"\"\"\n",
        "    return \"NONE\" not in classification.upper()\n",
        "\n",
        "\n",
        "print(\"추론 함수 정의 완료\")"
      ],
      "metadata": {
        "id": "exoioWeNiuL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_query(user_query):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"입력: {user_query}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    classification = classify_ambiguity(user_query)\n",
        "    print(f\"분류 결과: {classification}\")\n",
        "\n",
        "    if not is_ambiguous(classification):\n",
        "        print(\"라우팅: 답변 생성\\n\")\n",
        "        output = generate_answer(user_query)\n",
        "    else:\n",
        "        print(\"라우팅: 명확화 질문 생성\\n\")\n",
        "        output = generate_clarifying_question(classification, user_query)\n",
        "\n",
        "    print(f\"최종 답변: {output}\\n\")\n",
        "\n",
        "    return {\n",
        "        \"분류 결과\": classification,\n",
        "        \"출력\": output\n",
        "    }\n",
        "\n",
        "print(\"파이프라인 함수 정의 완료\")"
      ],
      "metadata": {
        "id": "DNs5hzjQiyEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo Interface\n",
        "\n",
        "Gradio 기반 인터랙티브 데모입니다. 질문을 입력하면:\n",
        "1. 모호성 분류 수행\n",
        "2. 분류 결과에 따라 명확화 질문 또는 직접 답변 생성"
      ],
      "metadata": {
        "id": "XS77LxBqTu9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def gradio_interface(query):\n",
        "    result = process_query(query)\n",
        "    return (\n",
        "        result[\"분류 결과\"],\n",
        "        result[\"출력\"]\n",
        "    )\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=gr.Textbox(\n",
        "        label=\"질문 입력\",\n",
        "        placeholder=\"예: Give me a list of good coffee shops?\",\n",
        "        lines=2\n",
        "    ),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"분류 결과\"),\n",
        "        gr.Textbox(label=\"최종 답변\", lines=5)\n",
        "    ],\n",
        "    title=\"Ambiguous Query Handler\",\n",
        "    description=\"모호한 질문을 분류하고 명확화 질문 또는 답변을 생성합니다.\",\n",
        "    examples=[\n",
        "        [\"Give me a list of good coffee shops?\"],\n",
        "        [\"What time does the store open?\"],\n",
        "        [\"What is the capital of France?\"],\n",
        "    ]\n",
        ")\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "GTlc6QvAi0ub"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}